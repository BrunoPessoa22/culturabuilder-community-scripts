# Ollama — 100% grátis (roda local, mais lento em CPU)
# Instalar: curl -fsSL https://ollama.com/install.sh | sh
LLM_PROVIDER=ollama
LLM_MODEL=llama3.1:8b
OLLAMA_BASE_URL=http://localhost:11434

# Modelos recomendados:
# llama3.1:8b  — Geral, ~5GB RAM
# mistral:7b   — Rápido, ~4.5GB RAM
# qwen2.5:7b   — Bom pra código, ~5GB RAM
# ⚠️ Railway não tem GPU — vai rodar em CPU (~5-10 tokens/s)
